# -*- coding: utf-8 -*-
"""Abb_case_study_prakhar

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BpwELb8kFJ5iBxihyxw_hwPQIc_B5015
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

df = pd.read_csv("My Drive/Colab Notebooks/train_Abb.csv")

print(df)

print(df.info())

print(df.describe())

print(df.isnull().sum())

df['Item_Weight'] = df['Item_Weight'].fillna(df['Item_Weight'].mean())
df['Outlet_Size'] = df['Outlet_Size'].fillna(df['Outlet_Size'].mode()[0])

df['Outlet_Age'] = 2013 - df['Outlet_Establishment_Year']
df.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

le = LabelEncoder()
df['Item_Fat_Content'] = le.fit_transform(df['Item_Fat_Content'])
df['Item_Type'] = le.fit_transform(df['Item_Type'])
df['Outlet_Identifier'] = le.fit_transform(df['Outlet_Identifier'])
df['Outlet_Size'] = le.fit_transform(df['Outlet_Size'])
df['Outlet_Location_Type'] = le.fit_transform(df['Outlet_Location_Type'])
df['Outlet_Type'] = le.fit_transform(df['Outlet_Type'])

print(df)

X = df.drop(['Item_Outlet_Sales','Item_Identifier'],axis=1)
y = df['Item_Outlet_Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=25)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test,y_pred))

print(rmse)

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=500, random_state=25 )

rf.fit(X_train,y_train)

y_pred_rf = rf.predict(X_test)

rmse_rf = np.sqrt(mean_squared_error(y_test,y_pred_rf))

print(rmse_rf)

testDataFrame_original =  pd.read_csv("My Drive/Colab Notebooks/test_Abb.csv")

testDataFrame = testDataFrame_original.copy(deep=True)

print(testDataFrame.isna().sum())

testDataFrame['Item_Weight'] = testDataFrame['Item_Weight'].fillna(testDataFrame['Item_Weight'].mean())
testDataFrame['Outlet_Size'] = testDataFrame['Outlet_Size'].fillna(testDataFrame['Outlet_Size'].mode()[0])

testDataFrame['Outlet_Age'] = 2013 - testDataFrame['Outlet_Establishment_Year']
testDataFrame.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

testDataFrame['Item_Fat_Content'] = le.fit_transform(testDataFrame['Item_Fat_Content'])
testDataFrame['Item_Type'] = le.fit_transform(testDataFrame['Item_Type'])
testDataFrame['Outlet_Identifier'] = le.fit_transform(testDataFrame['Outlet_Identifier'])
testDataFrame['Outlet_Size'] = le.fit_transform(testDataFrame['Outlet_Size'])
testDataFrame['Outlet_Location_Type'] = le.fit_transform(testDataFrame['Outlet_Location_Type'])
testDataFrame['Outlet_Type'] = le.fit_transform(testDataFrame['Outlet_Type'])

X_testDataFrame = testDataFrame.drop(['Item_Identifier'],axis=1)
# y_testDataFrame = testDataFrame['Item_Outlet_Sales']

y_pred_on_X_testDataFrame = rf.predict(X_testDataFrame)

FinalSubmission = pd.DataFrame({'Item_Identifier':testDataFrame_original['Item_Identifier'],'Outlet_Identifier':testDataFrame_original['Outlet_Identifier'],'Item_Outlet_Sales':y_pred_on_X_testDataFrame})

print(FinalSubmission)

FinalSubmission.to_csv('My Drive/Colab Notebooks/finalSubmission_v1.csv',index=False)





# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns

# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense
# from tensorflow.keras.callbacks import EarlyStopping

# # Load data
# df = pd.read_csv("My Drive/Colab Notebooks/train_Abb.csv")

# # Data preprocessing
# df['Item_Weight'] = df['Item_Weight'].fillna(df['Item_Weight'].mean())
# df['Outlet_Size'] = df['Outlet_Size'].fillna(df['Outlet_Size'].mode()[0])
# df['Outlet_Age'] = 2013 - df['Outlet_Establishment_Year']
# df.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# le = LabelEncoder()
# df['Item_Fat_Content'] = le.fit_transform(df['Item_Fat_Content'])
# df['Item_Type'] = le.fit_transform(df['Item_Type'])
# df['Outlet_Identifier'] = le.fit_transform(df['Outlet_Identifier'])
# df['Outlet_Size'] = le.fit_transform(df['Outlet_Size'])
# df['Outlet_Location_Type'] = le.fit_transform(df['Outlet_Location_Type'])
# df['Outlet_Type'] = le.fit_transform(df['Outlet_Type'])

# # Feature-target split
# X = df.drop(['Item_Outlet_Sales', 'Item_Identifier'], axis=1)
# y = df['Item_Outlet_Sales']

# # Train-test split
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)

# # Neural Network Model
# model = Sequential()
# model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(1))  # Output layer for regression

# model.compile(optimizer='adam', loss='mse')

# # Early stopping to prevent overfitting
# early_stop = EarlyStopping(monitor='val_loss', patience=10)

# # Training
# model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stop], verbose=1)

# # Prediction and Evaluation
# y_pred_nn = model.predict(X_test).flatten()
# rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))
# print("Neural Network RMSE:", rmse_nn)

# # Preprocess test data
# testDataFrame_original = pd.read_csv("My Drive/Colab Notebooks/test_Abb.csv")
# testDataFrame = testDataFrame_original.copy(deep=True)

# testDataFrame['Item_Weight'] = testDataFrame['Item_Weight'].fillna(testDataFrame['Item_Weight'].mean())
# testDataFrame['Outlet_Size'] = testDataFrame['Outlet_Size'].fillna(testDataFrame['Outlet_Size'].mode()[0])
# testDataFrame['Outlet_Age'] = 2013 - testDataFrame['Outlet_Establishment_Year']
# testDataFrame.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# testDataFrame['Item_Fat_Content'] = le.fit_transform(testDataFrame['Item_Fat_Content'])
# testDataFrame['Item_Type'] = le.fit_transform(testDataFrame['Item_Type'])
# testDataFrame['Outlet_Identifier'] = le.fit_transform(testDataFrame['Outlet_Identifier'])
# testDataFrame['Outlet_Size'] = le.fit_transform(testDataFrame['Outlet_Size'])
# testDataFrame['Outlet_Location_Type'] = le.fit_transform(testDataFrame['Outlet_Location_Type'])
# testDataFrame['Outlet_Type'] = le.fit_transform(testDataFrame['Outlet_Type'])

# X_testDataFrame = testDataFrame.drop(['Item_Identifier'], axis=1)

# # Predicting with neural network
# y_pred_final_nn = model.predict(X_testDataFrame).flatten()

# y_pred_final_nn = np.clip(y_pred_final_nn, 0, None)

# FinalSubmission = pd.DataFrame({
#     'Item_Identifier': testDataFrame_original['Item_Identifier'],
#     'Outlet_Identifier': testDataFrame_original['Outlet_Identifier'],
#     'Item_Outlet_Sales': y_pred_final_nn
# })

# print(FinalSubmission)

# FinalSubmission.to_csv('My Drive/Colab Notebooks/finalSubmission_nn_v2.csv', index=False)



#########################Output of Following code has been submitted Finally#########################

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Load data
df = pd.read_csv("My Drive/Colab Notebooks/train_Abb.csv")

# Data preprocessing
df['Item_Weight'] = df['Item_Weight'].fillna(df['Item_Weight'].mean())
df['Outlet_Size'] = df['Outlet_Size'].fillna(df['Outlet_Size'].mode()[0])
df['Outlet_Age'] = 2013 - df['Outlet_Establishment_Year']
df.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# Label Encoding
def encode_features(df, encoders=None):
    if encoders is None:
        encoders = {}
        for col in ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier',
                    'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            encoders[col] = le
    else:
        for col, le in encoders.items():
            df[col] = le.transform(df[col])
    return df, encoders

df, encoders = encode_features(df)

# Feature-target split
X = df.drop(['Item_Outlet_Sales', 'Item_Identifier'], axis=1)
y = df['Item_Outlet_Sales']

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA (retain 95% variance)
pca = PCA(n_components=0.95, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=25)

# Neural Network Model
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse')

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10)

# Training
model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32,
          callbacks=[early_stop], verbose=1)

# Prediction and Evaluation
y_pred_nn = model.predict(X_test).flatten()
rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))
print("Neural Network RMSE:", rmse_nn)

# Random Forest Regressor for comparison
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
}
rf = RandomForestRegressor(random_state=42)
grid_rf = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_rf.fit(X_train, y_train)

y_pred_rf = grid_rf.predict(X_test)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
print("Random Forest RMSE:", rmse_rf)

# Load test data
testDataFrame_original = pd.read_csv("My Drive/Colab Notebooks/test_Abb.csv")
testDataFrame = testDataFrame_original.copy(deep=True)

testDataFrame['Item_Weight'] = testDataFrame['Item_Weight'].fillna(testDataFrame['Item_Weight'].mean())
testDataFrame['Outlet_Size'] = testDataFrame['Outlet_Size'].fillna(testDataFrame['Outlet_Size'].mode()[0])
testDataFrame['Outlet_Age'] = 2013 - testDataFrame['Outlet_Establishment_Year']
testDataFrame.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

testDataFrame, _ = encode_features(testDataFrame, encoders)

X_testDataFrame = testDataFrame.drop(['Item_Identifier'], axis=1)
X_testDataFrame_scaled = scaler.transform(X_testDataFrame)
X_testDataFrame_pca = pca.transform(X_testDataFrame_scaled)

# Predicting with best model (choose based on lower RMSE)
final_model = model if rmse_nn <= rmse_rf else grid_rf
y_pred_final = final_model.predict(X_testDataFrame_pca)
if hasattr(y_pred_final, 'flatten'):
    y_pred_final = y_pred_final.flatten()

# Ensure sales predictions are positive
y_pred_final = np.clip(y_pred_final, 0, None)

FinalSubmission = pd.DataFrame({
    'Item_Identifier': testDataFrame_original['Item_Identifier'],
    'Outlet_Identifier': testDataFrame_original['Outlet_Identifier'],
    'Item_Outlet_Sales': y_pred_final
})

print(FinalSubmission)

FinalSubmission.to_csv('My Drive/Colab Notebooks/finalSubmission_nn_rf_v2.csv', index=False)



# # OneHotEncoder
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns

# from sklearn.preprocessing import StandardScaler
# from sklearn.decomposition import PCA
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import mean_squared_error

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout
# from tensorflow.keras.callbacks import EarlyStopping

# # Load training data
# df = pd.read_csv("My Drive/Colab Notebooks/train_Abb.csv")

# # Fill missing values
# df['Item_Weight'] = df['Item_Weight'].fillna(df['Item_Weight'].mean())
# df['Outlet_Size'] = df['Outlet_Size'].fillna(df['Outlet_Size'].mode()[0])
# df['Outlet_Age'] = 2013 - df['Outlet_Establishment_Year']
# df.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# # One-Hot Encoding
# df = pd.get_dummies(df, columns=['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier',
#                                  'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type'])

# # Feature-target split
# X = df.drop(['Item_Outlet_Sales', 'Item_Identifier'], axis=1)
# y = df['Item_Outlet_Sales']

# # Feature scaling
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# # Optional: Apply PCA (retain 95% variance)
# apply_pca = True
# if apply_pca:
#     pca = PCA(n_components=0.95, random_state=42)
#     X_final = pca.fit_transform(X_scaled)
# else:
#     X_final = X_scaled

# # Train-test split
# X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=25)

# # Neural Network Model
# model = Sequential()
# model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
# model.add(Dropout(0.3))
# model.add(Dense(64, activation='relu'))
# model.add(Dropout(0.3))
# model.add(Dense(1))

# model.compile(optimizer='adam', loss='mse')

# # Early stopping
# early_stop = EarlyStopping(monitor='val_loss', patience=10)

# # Training
# model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32,
#           callbacks=[early_stop], verbose=1)

# # Prediction and Evaluation
# y_pred_nn = model.predict(X_test).flatten()
# rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))
# print("Neural Network RMSE:", rmse_nn)

# # Random Forest Regressor for comparison
# param_grid = {
#     'n_estimators': [100, 200],
#     'max_depth': [10, 20, None],
#     'min_samples_split': [2, 5],
# }
# rf = RandomForestRegressor(random_state=42)
# grid_rf = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
# grid_rf.fit(X_train, y_train)

# y_pred_rf = grid_rf.predict(X_test)
# rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
# print("Random Forest RMSE:", rmse_rf)

# # Load and process test data
# testDataFrame_original = pd.read_csv("My Drive/Colab Notebooks/test_Abb.csv")
# testDataFrame = testDataFrame_original.copy(deep=True)

# testDataFrame['Item_Weight'] = testDataFrame['Item_Weight'].fillna(testDataFrame['Item_Weight'].mean())
# testDataFrame['Outlet_Size'] = testDataFrame['Outlet_Size'].fillna(testDataFrame['Outlet_Size'].mode()[0])
# testDataFrame['Outlet_Age'] = 2013 - testDataFrame['Outlet_Establishment_Year']
# testDataFrame.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# # Apply the same one-hot encoding
# testDataFrame = pd.get_dummies(testDataFrame, columns=['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier',
#                                                        'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type'])

# # Align test set with training features
# X, testDataFrame = X.align(testDataFrame, join='left', axis=1, fill_value=0)

# X_testDataFrame_scaled = scaler.transform(testDataFrame)
# if apply_pca:
#     X_testDataFrame_final = pca.transform(X_testDataFrame_scaled)
# else:
#     X_testDataFrame_final = X_testDataFrame_scaled

# # Predict using best model
# final_model = model if rmse_nn <= rmse_rf else grid_rf
# y_pred_final = final_model.predict(X_testDataFrame_final)
# if hasattr(y_pred_final, 'flatten'):
#     y_pred_final = y_pred_final.flatten()

# # Ensure no negative sales
# y_pred_final = np.clip(y_pred_final, 0, None)

# # Prepare submission
# FinalSubmission = pd.DataFrame({
#     'Item_Identifier': testDataFrame_original['Item_Identifier'],
#     'Outlet_Identifier': testDataFrame_original['Outlet_Identifier'],
#     'Item_Outlet_Sales': y_pred_final
# })

# print(FinalSubmission)

# FinalSubmission.to_csv('My Drive/Colab Notebooks/finalSubmission_ohe_nn_rf.csv', index=False)







# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns

# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
# from xgboost import XGBRegressor
# from sklearn.linear_model import Ridge

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense
# from tensorflow.keras.callbacks import EarlyStopping

# # Load data
# df = pd.read_csv("My Drive/Colab Notebooks/train_Abb.csv")

# # Preprocessing
# df['Item_Weight'].fillna(df['Item_Weight'].mean(), inplace=True)
# df['Outlet_Size'].fillna(df['Outlet_Size'].mode()[0], inplace=True)
# df['Outlet_Age'] = 2013 - df['Outlet_Establishment_Year']
# df.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# le = LabelEncoder()
# cols_to_encode = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']
# for col in cols_to_encode:
#     df[col] = le.fit_transform(df[col])

# X = df.drop(['Item_Outlet_Sales', 'Item_Identifier'], axis=1)
# y = df['Item_Outlet_Sales']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # ------------------ Base Model 1: Neural Network ------------------
# nn_model = Sequential([
#     Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
#     Dense(64, activation='relu'),
#     Dense(1)
# ])
# nn_model.compile(optimizer='adam', loss='mse')
# early_stop = EarlyStopping(monitor='val_loss', patience=10)
# nn_model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)
# y_pred_nn_train = nn_model.predict(X_train).flatten()
# y_pred_nn_test = nn_model.predict(X_test).flatten()

# # ------------------ Base Model 2: Random Forest ------------------
# rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
# rf.fit(X_train, y_train)
# y_pred_rf_train = rf.predict(X_train)
# y_pred_rf_test = rf.predict(X_test)

# # ------------------ Base Model 3: XGBoost ------------------
# xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
# xgb.fit(X_train, y_train)
# y_pred_xgb_train = xgb.predict(X_train)
# y_pred_xgb_test = xgb.predict(X_test)

# # ------------------ Create Meta Features ------------------
# meta_X_train = np.column_stack((y_pred_nn_train, y_pred_rf_train, y_pred_xgb_train))
# meta_X_test = np.column_stack((y_pred_nn_test, y_pred_rf_test, y_pred_xgb_test))

# # ------------------ Meta Model: Ridge Regression ------------------
# meta_model = Ridge(alpha=1.0)
# meta_model.fit(meta_X_train, y_train)
# meta_pred = meta_model.predict(meta_X_test)

# # ------------------ Evaluation ------------------
# rmse_meta = np.sqrt(mean_squared_error(y_test, meta_pred))
# print("Meta Model RMSE:", rmse_meta)

# # ------------------ Final Test Predictions ------------------
# test_df = pd.read_csv("My Drive/Colab Notebooks/test_Abb.csv")
# test_df['Item_Weight'].fillna(test_df['Item_Weight'].mean(), inplace=True)
# test_df['Outlet_Size'].fillna(test_df['Outlet_Size'].mode()[0], inplace=True)
# test_df['Outlet_Age'] = 2013 - test_df['Outlet_Establishment_Year']
# test_df.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# for col in cols_to_encode:
#     test_df[col] = le.fit_transform(test_df[col])

# X_final_test = test_df.drop(['Item_Identifier'], axis=1)

# final_nn = nn_model.predict(X_final_test).flatten()
# final_rf = rf.predict(X_final_test)
# final_xgb = xgb.predict(X_final_test)

# meta_features_test = np.column_stack((final_nn, final_rf, final_xgb))
# final_predictions = meta_model.predict(meta_features_test)
# final_predictions = np.clip(final_predictions, 0, None)

# # ------------------ Submission ------------------
# submission = pd.DataFrame({
#     'Item_Identifier': test_df['Item_Identifier'],
#     'Outlet_Identifier': test_df['Outlet_Identifier'],
#     'Item_Outlet_Sales': final_predictions
# })
# submission.to_csv('My Drive/Colab Notebooks/final_submission_meta_model_v3.csv', index=False)

# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns

# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error
# from sklearn.ensemble import RandomForestRegressor
# from xgboost import XGBRegressor
# from sklearn.linear_model import Ridge

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense
# from tensorflow.keras.callbacks import EarlyStopping

# # ------------------ Load Training Data ------------------
# df = pd.read_csv("My Drive/Colab Notebooks/train_Abb.csv")

# # ------------------ Preprocessing Training Data ------------------
# df['Item_Weight'].fillna(df['Item_Weight'].mean(), inplace=True)
# df['Outlet_Size'].fillna(df['Outlet_Size'].mode()[0], inplace=True)
# df['Outlet_Age'] = 2013 - df['Outlet_Establishment_Year']
# df.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# le = LabelEncoder()
# cols_to_encode = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']
# for col in cols_to_encode:
#     df[col] = le.fit_transform(df[col])

# X = df.drop(['Item_Outlet_Sales', 'Item_Identifier'], axis=1)
# y = df['Item_Outlet_Sales']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # ------------------ Base Model 1: Neural Network ------------------
# nn_model = Sequential([
#     Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
#     Dense(64, activation='relu'),
#     Dense(1)
# ])
# nn_model.compile(optimizer='adam', loss='mse')
# early_stop = EarlyStopping(monitor='val_loss', patience=10)
# nn_model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stop], verbose=0)
# y_pred_nn_train = nn_model.predict(X_train).flatten()
# y_pred_nn_test = nn_model.predict(X_test).flatten()

# # ------------------ Base Model 2: Random Forest ------------------
# rf = RandomForestRegressor(n_estimators=250, max_depth=10, random_state=42)
# rf.fit(X_train, y_train)
# y_pred_rf_train = rf.predict(X_train)
# y_pred_rf_test = rf.predict(X_test)

# # ------------------ Base Model 3: XGBoost ------------------
# xgb = XGBRegressor(n_estimators=250, learning_rate=0.1, max_depth=5, random_state=42)
# xgb.fit(X_train, y_train)
# y_pred_xgb_train = xgb.predict(X_train)
# y_pred_xgb_test = xgb.predict(X_test)

# # ------------------ Create Meta Features ------------------
# meta_X_train = np.column_stack((y_pred_nn_train, y_pred_rf_train, y_pred_xgb_train))
# meta_X_test = np.column_stack((y_pred_nn_test, y_pred_rf_test, y_pred_xgb_test))

# # ------------------ Meta Model: Ridge Regression ------------------
# meta_model = Ridge(alpha=1.0)
# meta_model.fit(meta_X_train, y_train)
# meta_pred = meta_model.predict(meta_X_test)

# # ------------------ Evaluation ------------------
# rmse_meta = np.sqrt(mean_squared_error(y_test, meta_pred))
# print("Meta Model RMSE:", rmse_meta)

# # ------------------ Load Test Data ------------------
# test_df = pd.read_csv("My Drive/Colab Notebooks/test_Abb.csv")

# # ------------------ Preprocessing Test Data ------------------
# test_df['Item_Weight'].fillna(test_df['Item_Weight'].mean(), inplace=True)
# test_df['Outlet_Size'].fillna(test_df['Outlet_Size'].mode()[0], inplace=True)
# test_df['Outlet_Age'] = 2013 - test_df['Outlet_Establishment_Year']
# test_df.drop(['Outlet_Establishment_Year'], axis=1, inplace=True)

# # Preserve original Outlet_Identifier for submission
# original_outlet_ids = test_df['Outlet_Identifier'].copy()

# # Apply same LabelEncoder transformation
# for col in cols_to_encode:
#     test_df[col] = le.fit_transform(test_df[col])

# X_final_test = test_df.drop(['Item_Identifier'], axis=1)

# # ------------------ Final Test Predictions ------------------
# final_nn = nn_model.predict(X_final_test).flatten()
# final_rf = rf.predict(X_final_test)
# final_xgb = xgb.predict(X_final_test)

# meta_features_test = np.column_stack((final_nn, final_rf, final_xgb))
# final_predictions = meta_model.predict(meta_features_test)
# final_predictions = np.clip(final_predictions, 0, None)

# # ------------------ Submission ------------------
# submission = pd.DataFrame({
#     'Item_Identifier': test_df['Item_Identifier'],
#     'Outlet_Identifier': original_outlet_ids,
#     'Item_Outlet_Sales': final_predictions
# })
# submission.to_csv('My Drive/Colab Notebooks/final_submission_meta_model_v3.csv', index=False)

